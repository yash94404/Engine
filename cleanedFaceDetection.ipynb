{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBGxyg66ArW2aEy0NpW4YP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash94404/Engine/blob/main/cleanedFaceDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"images\")"
      ],
      "metadata": {
        "id": "xHlbgHFjUgPJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "import os\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "from insightface.data import get_image as ins_get_image\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "\n",
        "# STEP 2: Create an FaceDetector object.\n",
        "base_options = python.BaseOptions(model_asset_path='detector.tflite')\n",
        "options = vision.FaceDetectorOptions(base_options=base_options, min_detection_confidence = 0.75)\n",
        "detector = vision.FaceDetector.create_from_options(options)\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.75, min_tracking_confidence=0.75)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "\n",
        "app = FaceAnalysis()\n",
        "app.prepare(ctx_id=0, det_thresh=0.5)\n",
        "\n",
        "FONTS =cv2.FONT_HERSHEY_COMPLEX\n",
        "\n",
        "# face bounder indices\n",
        "FACE_OVAL=[ 10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103,67, 109]\n",
        "\n",
        "# lips indices for Landmarks\n",
        "LIPS=[ 61, 146, 91, 181, 84, 17, 314, 405, 321, 375,291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95,185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78 ]\n",
        "LOWER_LIPS =[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
        "UPPER_LIPS=[ 185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78]\n",
        "# Left eyes indices\n",
        "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
        "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
        "\n",
        "# right eyes indices\n",
        "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]\n",
        "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]"
      ],
      "metadata": {
        "id": "w5EBNcsuRY7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_frames_from_videos(video_paths: list, save_path, every_n_frames=20, debug=False):\n",
        "  '''\n",
        "  given list of video paths, save every n frames to save_path\n",
        "  '''\n",
        "  def save_frames(video_path, total_frames_saved):\n",
        "      vidcap = cv2.VideoCapture(video_path)\n",
        "      success, image = vidcap.read()\n",
        "      count = 0\n",
        "      while success:\n",
        "          if count % every_n_frames == 0:\n",
        "              # add padding to frame number\n",
        "              frame_num = str(count).zfill(6)\n",
        "              frame_path = os.path.join(save_path, f'{video_path.split(\"/\")[-1].split(\".\")[0]}_{frame_num}.jpg')\n",
        "              cv2.imwrite(frame_path, image) # save frame as JPEG file\n",
        "              if debug:\n",
        "                  print(f\"saved to {frame_path}\")\n",
        "              total_frames_saved += 1\n",
        "\n",
        "          success, image = vidcap.read()\n",
        "          #progress_bar.update(1)\n",
        "          count += 1\n",
        "\n",
        "      return total_frames_saved\n",
        "\n",
        "  total_frames_saved = 0\n",
        "\n",
        "  if not os.path.exists(save_path):\n",
        "      os.makedirs(save_path)\n",
        "\n",
        "  for video_path in video_paths:\n",
        "      total_frames_saved += save_frames(video_path, total_frames_saved)\n",
        "\n",
        "def genCroppedFace(img, bbox):\n",
        "  dframe= cv2.imread(img)\n",
        "  image_rows, image_cols, _ = dframe.shape\n",
        "  image_input = cv2.cvtColor(dframe, cv2.COLOR_BGR2RGB)\n",
        "  rect_start_point = (int(bbox[0]), int(bbox[1]))\n",
        "  rect_end_point = (int(bbox[2]), int(bbox[3]))\n",
        "  xleft,ytop= rect_start_point\n",
        "  xright,ybot=rect_end_point\n",
        "  crop_img = image_input[ytop: ybot, xleft: xright]\n",
        "  crop_img = cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR)\n",
        "  return crop_img\n",
        "\n",
        "def getCroppedMediapipe(files):\n",
        "  allCroppedFaces = []\n",
        "  #cropDict = {}\n",
        "  for filename in files:\n",
        "    #print(filename)\n",
        "    image = mp.Image.create_from_file(filename)\n",
        "    counter = 0\n",
        "    # STEP 4: Detect faces in the input image.\n",
        "    detection_result = detector.detect(image)\n",
        "    for detection in detection_result.detections:\n",
        "      # Draw bounding_box\n",
        "      bbox = detection.bounding_box\n",
        "      start_point = bbox.origin_x, bbox.origin_y\n",
        "      end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "      bbox = [start_point[0], start_point[1], end_point[0], end_point[1]]\n",
        "      croppedFace = genCroppedFace(filename, bbox)\n",
        "      cropfilename = filename.split('/')[0] + '/'+ \"cropped_\" + str(counter) + \"_\" + filename.split('/')[1]\n",
        "      #cropfilename = \"cropped \" + str(counter) + \" \" + filename\n",
        "      cv2.imwrite(cropfilename, croppedFace)\n",
        "      #cropDict[cropfilename] = filename\n",
        "      allCroppedFaces.append(cropfilename)\n",
        "      counter += 1\n",
        "  return allCroppedFaces\n",
        "def add_padding(filename, amt = 50): #\"./newstraightframes/cropped 0 i think you should leave season 2 trailer_000615.jpg\", 50\n",
        "  image = Image.open(filename)\n",
        "  right = amt\n",
        "  left = amt\n",
        "  top = amt\n",
        "  bottom = amt\n",
        "  width, height = image.size\n",
        "\n",
        "  new_width = width + right + left\n",
        "  new_height = height + top + bottom\n",
        "\n",
        "  result = Image.new(image.mode, (new_width, new_height), (255, 255, 255))\n",
        "\n",
        "  result.paste(image, (left, top))\n",
        "  result.save(filename)\n",
        "\n",
        "def euclaideanDistance(point, point1):\n",
        "    x, y = point\n",
        "    x1, y1 = point1\n",
        "    distance = math.sqrt((x1 - x)**2 + (y1 - y)**2)\n",
        "    return distance\n",
        "\n",
        "def blinkRatio(img, landmarks, right_indices, left_indices):\n",
        "    # Right eyes\n",
        "    # horizontal line\n",
        "    rh_right = landmarks[right_indices[0]]\n",
        "    rh_left = landmarks[right_indices[8]]\n",
        "    # vertical line\n",
        "    rv_top = landmarks[right_indices[12]]\n",
        "    rv_bottom = landmarks[right_indices[4]]\n",
        "\n",
        "    print(rh_right, rh_left, rv_top, rv_bottom)\n",
        "    right_eye_center = ((rh_right[0] + rh_left[0])/2, (rh_right[1]+rh_left[1]/2))\n",
        "    # draw lines on right eyes\n",
        "    # cv.line(img, rh_right, rh_left, utils.GREEN, 2)\n",
        "    # cv.line(img, rv_top, rv_bottom, utils.WHITE, 2)\n",
        "\n",
        "    # LEFT_EYE\n",
        "    # horizontal line\n",
        "    lh_right = landmarks[left_indices[0]]\n",
        "    lh_left = landmarks[left_indices[8]]\n",
        "\n",
        "    # vertical line\n",
        "    lv_top = landmarks[left_indices[12]]\n",
        "    lv_bottom = landmarks[left_indices[4]]\n",
        "\n",
        "    left_eye_center = ((lh_right[0] + lh_left[0])/2, (lh_right[1]+lh_left[1]/2))\n",
        "\n",
        "    left_eye_x = left_eye_center[0]\n",
        "    left_eye_y = left_eye_center[1]\n",
        "    right_eye_x = right_eye_center[0]\n",
        "    right_eye_y = right_eye_center[1]\n",
        "\n",
        "    delta_x = right_eye_x - left_eye_x\n",
        "    delta_y = right_eye_y - left_eye_y\n",
        "\n",
        "    # Slope of line formula\n",
        "    angle = np.arctan(delta_y / delta_x)\n",
        "\n",
        "    # Converting radians to degrees\n",
        "    angle = (angle * 180) / np.pi\n",
        "\n",
        "    # Provided a margin of error of 10 degrees\n",
        "    # (i.e, if the face tilts more than 10 degrees\n",
        "    # on either side the program will classify as right or left tilt)\n",
        "    eyeDirection = \"\"\n",
        "    if angle > 10:\n",
        "        eyeDirection = \"Right\"\n",
        "        #cv2.putText(img, 'RIGHT TILT :' + str(int(angle))+' degrees',\n",
        "         #              (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "         #              (0, 0, 0), 2, cv2.LINE_4)\n",
        "    elif angle < -10:\n",
        "        eyeDirection = \"Left\"\n",
        "        #cv2.putText(img, 'LEFT TILT :' + str(int(angle))+' degrees',\n",
        "        #               (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "        #               (0, 0, 0), 2, cv2.LINE_4)\n",
        "    else:\n",
        "        eyeDirection = \"Straight\"\n",
        "        #cv2.putText(img, 'STRAIGHT :', (20, 30),\n",
        "        #               cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "        #               (0, 0, 0), 2, cv2.LINE_4)\n",
        "\n",
        "    #cv2_imshow(img)\n",
        "\n",
        "    rhDistance = euclaideanDistance(rh_right, rh_left)\n",
        "    rvDistance = euclaideanDistance(rv_top, rv_bottom)\n",
        "\n",
        "    lvDistance = euclaideanDistance(lv_top, lv_bottom)\n",
        "    lhDistance = euclaideanDistance(lh_right, lh_left)\n",
        "\n",
        "    if rvDistance == 0 or lvDistance == 0:\n",
        "      return 100, \"bad\"\n",
        "    reRatio = rhDistance/rvDistance\n",
        "    leRatio = lhDistance/lvDistance\n",
        "\n",
        "    ratio = (reRatio+leRatio)/2\n",
        "    return ratio, eyeDirection\n",
        "\n",
        "def findFacePose(image_path):\n",
        "  #print(image_path)\n",
        "  lookingForward = True\n",
        "  eyesOpen = True\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "  image.flags.writeable = False\n",
        "  results = face_mesh.process(image)\n",
        "  image.flags.writeable = True\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "  img_h, img_w, img_c = image.shape\n",
        "  #print(image.shape)\n",
        "  face_3d = []\n",
        "  face_2d = []\n",
        "  mesh_coord = []\n",
        "  if results.multi_face_landmarks:\n",
        "    for face_landmarks in results.multi_face_landmarks:\n",
        "      for idx, lm in enumerate(face_landmarks.landmark):\n",
        "        if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
        "          if idx == 1:\n",
        "            nose_2d = (lm.x * img_w, lm.y * img_h)\n",
        "            nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
        "\n",
        "          x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
        "          face_2d.append([x,y])\n",
        "          face_3d.append([x,y,lm.z])\n",
        "        mesh_coord.append((int(lm.x*img_w), int(lm.y*img_h)))\n",
        "      face_2d = np.array(face_2d, dtype = np.float64)\n",
        "      face_3d = np.array(face_3d, dtype= np.float64)\n",
        "\n",
        "      #print(face_3d)\n",
        "      focal_length = 1 * img_w\n",
        "      #print(\"FOCAL LENGTH IS \", focal_length)\n",
        "      cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
        "                                [0, focal_length, img_w / 2],\n",
        "                                  [0, 0, 1]])\n",
        "      #print(dist_matrix)\n",
        "      dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
        "      #print(face_3d, type(face_3d))\n",
        "      #print(face_2d, type(face_2d))\n",
        "      #print(cam_matrix, type(cam_matrix[0][0]))\n",
        "      #print(dist_matrix, type(dist_matrix))\n",
        "      success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
        "      rmat, jac = cv2.Rodrigues(rot_vec)\n",
        "      angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
        "      x = angles[0] * 360\n",
        "      y = angles[1] * 360\n",
        "      z = angles[2] * 360\n",
        "      #print(x, y, z)\n",
        "      # See where the user's head tilting\n",
        "      if y < -30 or y > 30 or x < -30 or x > 30:\n",
        "        lookingForward = False\n",
        "\n",
        "      ratio, eyeDirection = blinkRatio(image, mesh_coord, RIGHT_EYE, LEFT_EYE)\n",
        "      #return ratio\n",
        "      if ratio > 4:\n",
        "        eyesOpen = False\n",
        "      #print(text)\n",
        "      # Display the nose direction\n",
        "      #nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
        "      #print(nose_3d)\n",
        "      #rint(nose_3d_projection)\n",
        "      #p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
        "      #p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))\n",
        "      #print(p1, p2)\n",
        "      #cv2.line(image, p1, p2, (255, 0, 0), 3)\n",
        "\n",
        "      # Add the text on the image\n",
        "      #cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
        "      #cv2.putText(image, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "      #cv2.putText(image, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "      #cv2.putText(image, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "    #cv2_imshow(image)\n",
        "    return (lookingForward, x, y, z, eyesOpen, ratio, eyeDirection)\n",
        "def findAllStraight(ims):\n",
        "  allStraight = []\n",
        "  acceptableFrames = []\n",
        "  for im in ims:\n",
        "    tup = findFacePose(im)\n",
        "    print(tup)\n",
        "    if tup and tup[0]:\n",
        "      acceptableFrames.append(im)\n",
        "      if tup[0] and tup[4] and tup[6] == \"Straight\":\n",
        "        allStraight.append(im)\n",
        "        print(\"x = \", tup[1], \", y = \", tup[2])\n",
        "  return allStraight, acceptableFrames\n",
        "from sklearn import cluster\n",
        "def generateClustersNew(files, dataset_path):\n",
        "  accepted_files = []\n",
        "  features = []\n",
        "  counter = 1\n",
        "  #newfiles = set()\n",
        "  #for file in files:\n",
        "  #  newfiles.add(cropDict[file])\n",
        "  for file in files:\n",
        "    if not file.endswith('.jpg'):\n",
        "      continue\n",
        "    #print('processing image %d: %s'%(counter,file))\n",
        "\n",
        "    #img = cv2.imread(os.path.join(dataset_path, file))\n",
        "    img = cv2.imread(file)\n",
        "    #cv2_imshow(img)\n",
        "\n",
        "    faces = app.get(img)\n",
        "    if not faces:\n",
        "      #print(\"HI\")\n",
        "      continue\n",
        "    #print(faces[0])\n",
        "    print(len(faces))\n",
        "    #print(\"_________\")\n",
        "    features.append(faces[0].normed_embedding)\n",
        "\n",
        "    accepted_files.append(file)\n",
        "\n",
        "    counter+=1\n",
        "  #y_pred = cluster.DBSCAN(eps=1, min_samples=2).fit_predict(features)\n",
        "  #print(accepted_files)\n",
        "  #print(y_pred)\n",
        "  #show_clustering_result(y_pred, dataset_path, accepted_files)\n",
        "  print(features)\n",
        "  y_pred = cluster.AgglomerativeClustering(n_clusters=None, distance_threshold=1, linkage='single').fit_predict(features)\n",
        "  #print(accepted_files)\n",
        "  #print(y_pred)\n",
        "\n",
        "  clusterDict = {}\n",
        "  for i in range(len(y_pred)):\n",
        "    if y_pred[i] not in clusterDict.keys():\n",
        "      clusterDict[y_pred[i]] = list()\n",
        "    clusterDict[y_pred[i]].append(accepted_files[i])\n",
        "  #print(clusterDict)\n",
        "  #show_clustering_result(y_pred, dataset_path, accepted_files)\n",
        "  return clusterDict"
      ],
      "metadata": {
        "id": "jmeMBcwPRcoZ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"images/\"\n",
        "video_files = [\"ninetyflownby.mp4\"]\n",
        "save_frames_from_videos(video_files, save_path, every_n_frames=15, debug=False)\n",
        "allFrames = []\n",
        "for filename in os.listdir('images/'):\n",
        "  if filename.endswith('.jpg'):\n",
        "    allFrames.append('images/' + filename)\n",
        "allCroppedFaces = getCroppedMediapipe(allFrames)\n",
        "for face in allCroppedFaces:\n",
        "  add_padding(face)\n",
        "#allStraight, acceptableFrames = findAllStraight(allCroppedFaces)\n",
        "#clusters = generateClustersNew(allStraight, \"./\")\n",
        "clusters = generateClustersNew(allCroppedFaces, \"./\")"
      ],
      "metadata": {
        "id": "9ntZTINXSQzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "for clusterID in clusters.keys():\n",
        "  print(clusterID)\n",
        "  for img in clusters[clusterID]:\n",
        "    print(img)\n",
        "    cv2_imshow(cv2.imread(img))"
      ],
      "metadata": {
        "id": "41AvG7iYVJkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clusters[2])"
      ],
      "metadata": {
        "id": "kSxAUWlIdHsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install viztracer"
      ],
      "metadata": {
        "id": "AVLUrJEJ6cki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizviewer"
      ],
      "metadata": {
        "id": "B34cgI_FaIJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!viztracer face_detect_full.py --source \"i_think_you_should_leave_season_2_trailer.mp4\""
      ],
      "metadata": {
        "id": "IXCFXIyH6iML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!vizviewer /content/result.json\n"
      ],
      "metadata": {
        "id": "jGzAHTZX80bS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}